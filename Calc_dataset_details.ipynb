{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbe770c",
   "metadata": {},
   "source": [
    "*package_leaflets_dataset_final.ipynb* from EZ --- creating json files in bioleaflets/dataset/test|train|valid   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec67f4",
   "metadata": {},
   "source": [
    "## JSON FILE\n",
    "\n",
    "            \n",
    "\n",
    "    leaflet_content = {      \n",
    "            'ID': leaflet.id,    \n",
    "            'URL': leaflet.url,    \n",
    "            'Product_Name': leaflet.product_name,    \n",
    "            'Full_Content': leaflet.content,    \n",
    "\n",
    "            'Section_1': {\n",
    "                'Title': leaflet.section1.title,\n",
    "                'Section_Content': leaflet.section1.section_content,\n",
    "                'Entity_Recognition': leaflet.section1.entity_recognition\n",
    "            },\n",
    "\n",
    "            'Section_2': {\n",
    "                'Title': leaflet.section2.title,\n",
    "                'Section_Content': leaflet.section2.section_content,\n",
    "                'Entity_Recognition': leaflet.section2.entity_recognition\n",
    "            },\n",
    "\n",
    "            'Section_3': {\n",
    "                'Title': leaflet.section3.title,\n",
    "                'Section_Content': leaflet.section3.section_content,\n",
    "                'Entity_Recognition': leaflet.section3.entity_recognition\n",
    "            },\n",
    "\n",
    "            'Section_4': {\n",
    "                'Title': leaflet.section4.title,\n",
    "                'Section_Content': leaflet.section4.section_content,\n",
    "                'Entity_Recognition': leaflet.section4.entity_recognition\n",
    "            },\n",
    "\n",
    "            'Section_5': {\n",
    "                'Title': leaflet.section5.title,\n",
    "                'Section_Content': leaflet.section5.section_content,\n",
    "                'Entity_Recognition': leaflet.section5.entity_recognition\n",
    "            },\n",
    "\n",
    "            'Section_6': {\n",
    "                'Title': leaflet.section6.title,\n",
    "                'Section_Content': leaflet.section6.section_content,\n",
    "                'Entity_Recognition': leaflet.section6.entity_recognition\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c862079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45729f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bioleaflets/dataset/test/0A1B8116D0DEC288D3F1CEBA70918447.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6261cd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ID', 'URL', 'Product_Name', 'Full_Content', 'Section_1', 'Section_2', 'Section_3', 'Section_4', 'Section_5', 'Section_6'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa35a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0A1B8116D0DEC288D3F1CEBA70918447'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6a874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Erleada'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Product_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fbcef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Title', 'Section_Content', 'Entity_Recognition'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Section_1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "967bf788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. what erleada is and what it is used for'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Section_1']['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3facde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'erleada is a cancer medicine that contains the active substance apalutamide. it is used to treat adult men with prostate cancer that: has metastasised to other parts of the body and still responds to medical or surgical treatments that lower testosterone (also called hormone-sensitive prostate cancer). has not metastasised to other parts of the body and no longer responds to medical or surgical treatment that lowers testosterone (also called castration-resistant prostate cancer). erleada works by blocking the activity of hormones called androgens (such as testosterone). androgens can cause the cancer to grow. by blocking the effect of androgens, apalutamide stops prostate cancer cells from growing and dividing.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Section_1']['Section_Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1951f",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff543b9",
   "metadata": {},
   "source": [
    "### Read all json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c44f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dataset(path='bioleaflets/dataset/test/'):\n",
    "    \n",
    "    dataset_array = []\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "\n",
    "        # path to the particular file\n",
    "        path2file = path + filename\n",
    "        \n",
    "        # read file\n",
    "        with open(path2file) as json_file:\n",
    "            test_file = json.load(json_file)\n",
    "        \n",
    "        # use less RAM by setting full_content = 0\n",
    "        test_file['Full_Content'] = None\n",
    "        \n",
    "        # save to array\n",
    "        dataset_array.append(test_file)\n",
    "    \n",
    "    return dataset_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd64937",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = read_dataset('bioleaflets/dataset/test/')\n",
    "val_dataset = read_dataset('bioleaflets/dataset/valid/')\n",
    "train_dataset = read_dataset('bioleaflets/dataset/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e14dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(test_dataset) == 134\n",
    "assert len(val_dataset) == 134\n",
    "assert len(train_dataset) == 1068\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa8b9e",
   "metadata": {},
   "source": [
    "## Check quiality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b02bd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1336"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dataset = test_dataset + val_dataset + train_dataset\n",
    "\n",
    "len(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aae749",
   "metadata": {},
   "source": [
    "### Count None values in each section type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "500e0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sections_None(dataset):\n",
    "    \"\"\"\n",
    "    check for NONE in section_content and/or NER_output\n",
    "    \"\"\"\n",
    "    \n",
    "    section_count_None = {'section_1_none': 0,\n",
    "                     'section_2_none': 0,\n",
    "                     'section_3_none': 0,\n",
    "                     'section_4_none': 0,\n",
    "                     'section_5_none': 0,\n",
    "                     'section_6_none': 0\n",
    "                    }\n",
    "    \n",
    "    NER_count_None = {'section_1_NER_none': 0,\n",
    "                     'section_2_NER_none': 0,\n",
    "                     'section_3_NER_none': 0,\n",
    "                     'section_4_NER_none': 0,\n",
    "                     'section_5_NER_none': 0,\n",
    "                     'section_6_NER_none': 0\n",
    "                    }\n",
    "    \n",
    "    for file in dataset:\n",
    "        \n",
    "        ### section contents\n",
    "        if file['Section_1']['Section_Content'] is None:\n",
    "            section_count_None['section_1_none'] += 1\n",
    "        \n",
    "        if file['Section_2']['Section_Content'] is None:\n",
    "            section_count_None['section_2_none'] += 1\n",
    "        \n",
    "        if file['Section_3']['Section_Content'] is None:\n",
    "            section_count_None['section_3_none'] += 1\n",
    "        \n",
    "        if file['Section_4']['Section_Content'] is None:\n",
    "            section_count_None['section_4_none'] += 1\n",
    "        \n",
    "        if file['Section_5']['Section_Content'] is None:\n",
    "            section_count_None['section_5_none'] += 1\n",
    "            \n",
    "        if file['Section_6']['Section_Content'] is None:\n",
    "            section_count_None['section_6_none'] += 1\n",
    "            \n",
    "        ### NER outputs\n",
    "        if file['Section_1']['Entity_Recognition'] is None:\n",
    "            NER_count_None['section_1_NER_none'] += 1\n",
    "        \n",
    "        if file['Section_2']['Entity_Recognition'] is None:\n",
    "            NER_count_None['section_2_NER_none'] += 1\n",
    "        \n",
    "        if file['Section_3']['Entity_Recognition'] is None:\n",
    "            NER_count_None['section_3_NER_none'] += 1\n",
    "        \n",
    "        if file['Section_4']['Entity_Recognition'] is None:\n",
    "            NER_count_None['section_4_NER_none'] += 1\n",
    "        \n",
    "        if file['Section_5']['Entity_Recognition'] is None:\n",
    "            NER_count_None['section_5_NER_none'] += 1\n",
    "            \n",
    "        if file['Section_6']['Entity_Recognition'] is None:\n",
    "            NER_count_None['section_6_NER_none'] += 1\n",
    "    \n",
    "    return section_count_None, NER_count_None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e51ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'section_1_none': 22,\n",
       "  'section_2_none': 27,\n",
       "  'section_3_none': 23,\n",
       "  'section_4_none': 41,\n",
       "  'section_5_none': 164,\n",
       "  'section_6_none': 25},\n",
       " {'section_1_NER_none': 15,\n",
       "  'section_2_NER_none': 15,\n",
       "  'section_3_NER_none': 11,\n",
       "  'section_4_NER_none': 31,\n",
       "  'section_5_NER_none': 472,\n",
       "  'section_6_NER_none': 22})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sections_None(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3656a23",
   "metadata": {},
   "source": [
    "### Count Empty values in each section type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e100b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sections_empty(dataset):\n",
    "    \"\"\"\n",
    "    check for empty in section_content and/or NER_output\n",
    "    \"\"\"\n",
    "    \n",
    "    section_count_empty = {'section_1_empty': 0,\n",
    "                     'section_2_empty': 0,\n",
    "                     'section_3_empty': 0,\n",
    "                     'section_4_empty': 0,\n",
    "                     'section_5_empty': 0,\n",
    "                     'section_6_empty': 0\n",
    "                    }\n",
    "    \n",
    "    NER_count_empty = {'section_1_NER_empty': 0,\n",
    "                     'section_2_NER_empty': 0,\n",
    "                     'section_3_NER_empty': 0,\n",
    "                     'section_4_NER_empty': 0,\n",
    "                     'section_5_NER_empty': 0,\n",
    "                     'section_6_NER_empty': 0\n",
    "                    }\n",
    "    \n",
    "    for file in dataset:\n",
    "        \n",
    "        ### section contents\n",
    "        if file['Section_1']['Section_Content'] is not None:\n",
    "            if len(file['Section_1']['Section_Content']) < 1:\n",
    "                section_count_empty['section_1_empty'] += 1\n",
    "        \n",
    "        if file['Section_2']['Section_Content'] is not None:\n",
    "            if len(file['Section_2']['Section_Content']) < 1:\n",
    "                section_count_empty['section_2_empty'] += 1\n",
    "        \n",
    "        if file['Section_3']['Section_Content'] is not None:\n",
    "            if len(file['Section_3']['Section_Content']) < 1:\n",
    "                section_count_empty['section_3_empty'] += 1\n",
    "        \n",
    "        if file['Section_4']['Section_Content'] is not None:\n",
    "            if len(file['Section_4']['Section_Content']) < 1:\n",
    "                section_count_empty['section_4_empty'] += 1\n",
    "        \n",
    "        if file['Section_5']['Section_Content'] is not None:\n",
    "            if len(file['Section_5']['Section_Content']) < 1:\n",
    "                section_count_empty['section_5_empty'] += 1\n",
    "            \n",
    "        if file['Section_6']['Section_Content'] is not None:\n",
    "            if len(file['Section_6']['Section_Content']) < 1:\n",
    "                section_count_empty['section_6_empty'] += 1\n",
    "            \n",
    "        ### NER outputs\n",
    "        if file['Section_1']['Entity_Recognition'] is not None:\n",
    "            if len(file['Section_1']['Entity_Recognition']) < 1:\n",
    "                NER_count_empty['section_1_NER_empty'] += 1\n",
    "        \n",
    "        if file['Section_2']['Entity_Recognition'] is not None:\n",
    "            if len(file['Section_2']['Entity_Recognition']) < 1:\n",
    "                NER_count_empty['section_2_NER_empty'] += 1\n",
    "        \n",
    "        if file['Section_3']['Entity_Recognition'] is not None:\n",
    "            if len(file['Section_3']['Entity_Recognition']) < 1:\n",
    "                NER_count_empty['section_3_NER_empty'] += 1\n",
    "        \n",
    "        if file['Section_4']['Entity_Recognition'] is not None:\n",
    "            if len(file['Section_4']['Entity_Recognition']) < 1:\n",
    "                NER_count_empty['section_4_NER_empty'] += 1\n",
    "        \n",
    "        if file['Section_5']['Entity_Recognition'] is not None:\n",
    "            if len(file['Section_5']['Entity_Recognition']) < 1:\n",
    "                NER_count_empty['section_5_NER_empty'] += 1\n",
    "            \n",
    "        if file['Section_6']['Entity_Recognition'] is not None:\n",
    "            if len(file['Section_6']['Entity_Recognition']) < 1:\n",
    "                NER_count_empty['section_6_NER_empty'] += 1\n",
    "    \n",
    "    return section_count_empty, NER_count_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27098e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'section_1_empty': 0,\n",
       "  'section_2_empty': 0,\n",
       "  'section_3_empty': 0,\n",
       "  'section_4_empty': 0,\n",
       "  'section_5_empty': 0,\n",
       "  'section_6_empty': 0},\n",
       " {'section_1_NER_empty': 0,\n",
       "  'section_2_NER_empty': 0,\n",
       "  'section_3_NER_empty': 0,\n",
       "  'section_4_NER_empty': 0,\n",
       "  'section_5_NER_empty': 0,\n",
       "  'section_6_NER_empty': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sections_empty(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298b4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdab8c51",
   "metadata": {},
   "source": [
    "### Check for duplicates string-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebeea507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_section(dataset):\n",
    "    \"\"\"\n",
    "    Test dataset for duplicates in section_content and NER_output for corresponding section_content\n",
    "    \"\"\"\n",
    "\n",
    "    # keep track of unique NER outputs observed so far\n",
    "    unique_NER_outputs = set()\n",
    "\n",
    "    # keep track of unique section contents observed so far\n",
    "    unique_sections = set()\n",
    "    \n",
    "    COUNT_DUPLICATE_NER_OUTPUTS = 0\n",
    "    COUNT_DUPLICATE_SECTIONS = 0\n",
    "    \n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        leaflet_sections = [leaflet['Section_1']['Section_Content'],\n",
    "                            leaflet['Section_2']['Section_Content'],\n",
    "                            leaflet['Section_3']['Section_Content'],\n",
    "                            leaflet['Section_4']['Section_Content'],\n",
    "                            leaflet['Section_5']['Section_Content'],\n",
    "                            leaflet['Section_6']['Section_Content']]\n",
    "\n",
    "        for section_index, section_content in enumerate(leaflet_sections):\n",
    "\n",
    "            if section_content is None:\n",
    "                continue\n",
    "            \n",
    "            # if section_content is already in set - unique_sections, then it is a duplicate\n",
    "            if section_content in unique_sections and len(section_content) > 1:\n",
    "                COUNT_DUPLICATE_SECTIONS += 1\n",
    "            # add section_content to a set\n",
    "            else:\n",
    "                unique_sections.add(section_content)\n",
    "        \n",
    "        ### check for dupicate NERs\n",
    "        \n",
    "        leaflet_NERs = [leaflet['Section_1']['Entity_Recognition'],\n",
    "                        leaflet['Section_2']['Entity_Recognition'],\n",
    "                        leaflet['Section_3']['Entity_Recognition'],\n",
    "                        leaflet['Section_4']['Entity_Recognition'],\n",
    "                        leaflet['Section_5']['Entity_Recognition'],\n",
    "                        leaflet['Section_6']['Entity_Recognition']]\n",
    "        \n",
    "        for section_index, NER_output in enumerate(leaflet_NERs):\n",
    "\n",
    "            if NER_output is None:\n",
    "                continue\n",
    "                        \n",
    "            # convert list of entitis to string of entities['Text']\n",
    "            NER_output_str = \"\"\n",
    "            for entity in NER_output:\n",
    "                NER_output_str += entity['Text'] + \" \"\n",
    "            \n",
    "            # if section_content is already in set - unique_sections, then it is a duplicate\n",
    "            if NER_output_str in unique_NER_outputs and len(NER_output) > 1:\n",
    "                COUNT_DUPLICATE_NER_OUTPUTS += 1\n",
    "            # add section_content to a set\n",
    "            else:\n",
    "                unique_NER_outputs.add(NER_output_str)\n",
    "        \n",
    "            \n",
    "    print('Num. of detected duplicate NER outputs:', COUNT_DUPLICATE_NER_OUTPUTS)\n",
    "    print('Num. of detected duplicate section contents:', COUNT_DUPLICATE_SECTIONS)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print('Num. of unique NER outputs:', len(unique_NER_outputs))\n",
    "    print('Num. of unique section contents:', len(unique_sections))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4bb924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of detected duplicate NER outputs: 0\n",
      "Num. of detected duplicate section contents: 0\n",
      "\n",
      "Num. of unique NER outputs: 7450\n",
      "Num. of unique section contents: 7714\n"
     ]
    }
   ],
   "source": [
    "check_duplicates_section(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1608e",
   "metadata": {},
   "source": [
    "Comment: 1336 * 6 = 8016, the rest are Nones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40730cd5",
   "metadata": {},
   "source": [
    "### TO-DO: check for dupicates in a smart way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb9d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5c4942f",
   "metadata": {},
   "source": [
    "## Cals Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a40145",
   "metadata": {},
   "source": [
    "### Calc number samples per section\n",
    "\n",
    "---> if section has a section1 and section_content is not None and is not Empty - count +1 to section1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23c2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sections(dataset):\n",
    "    \n",
    "    section_count = {'section_1_num': 0,\n",
    "                     'section_2_num': 0,\n",
    "                     'section_3_num': 0,\n",
    "                     'section_4_num': 0,\n",
    "                     'section_5_num': 0,\n",
    "                     'section_6_num': 0\n",
    "                    }\n",
    "    \n",
    "    for file in dataset:\n",
    "\n",
    "        if file['Section_1']['Section_Content'] is not None:\n",
    "            if len(file['Section_1']['Section_Content']) > 0:\n",
    "                section_count['section_1_num'] += 1\n",
    "        \n",
    "        if file['Section_2']['Section_Content'] is not None:\n",
    "            if len(file['Section_2']['Section_Content']) > 0:\n",
    "                section_count['section_2_num'] += 1\n",
    "        \n",
    "        if file['Section_3']['Section_Content'] is not None:\n",
    "            if len(file['Section_3']['Section_Content']) > 0:\n",
    "                section_count['section_3_num'] += 1\n",
    "        \n",
    "        if file['Section_4']['Section_Content'] is not None:\n",
    "            if len(file['Section_4']['Section_Content']) > 0:\n",
    "                section_count['section_4_num'] += 1\n",
    "        \n",
    "        if file['Section_5']['Section_Content'] is not None:\n",
    "            if len(file['Section_5']['Section_Content']) > 0:\n",
    "                section_count['section_5_num'] += 1\n",
    "            \n",
    "        if file['Section_6']['Section_Content'] is not None:\n",
    "            if len(file['Section_6']['Section_Content']) > 0:\n",
    "                section_count['section_6_num'] += 1\n",
    "        \n",
    "    return section_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac4e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_section_count = count_sections(test_dataset)\n",
    "val_section_count = count_sections(val_dataset)\n",
    "train_section_count = count_sections(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "559a5e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section_1_num': 133, 'section_2_num': 133, 'section_3_num': 133, 'section_4_num': 129, 'section_5_num': 115, 'section_6_num': 133}\n",
      "{'section_1_num': 130, 'section_2_num': 130, 'section_3_num': 134, 'section_4_num': 130, 'section_5_num': 124, 'section_6_num': 130}\n",
      "{'section_1_num': 1051, 'section_2_num': 1046, 'section_3_num': 1046, 'section_4_num': 1036, 'section_5_num': 933, 'section_6_num': 1048}\n",
      "\n",
      "\n",
      "section_1_num  :  1314\n",
      "section_2_num  :  1309\n",
      "section_3_num  :  1313\n",
      "section_4_num  :  1295\n",
      "section_5_num  :  1172\n",
      "section_6_num  :  1311\n"
     ]
    }
   ],
   "source": [
    "print(test_section_count)\n",
    "print(val_section_count)\n",
    "print(train_section_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for sec in ['section_1_num', 'section_2_num', 'section_3_num', 'section_4_num', 'section_5_num', 'section_6_num']:\n",
    "    print(sec, \" : \", test_section_count[sec] + val_section_count[sec] + train_section_count[sec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916cdbb5",
   "metadata": {},
   "source": [
    "### Calc average length char-wise of each section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19eb8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_section_len(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the average length of each section(1-6)\n",
    "    :param package_leaflets: list, collection of package leaflets\n",
    "    :return: dict, (key: str, section_num; value: list of lengths of corresponding sections)\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate length of each section\n",
    "\n",
    "    section_content_len = {\n",
    "        '1': [],\n",
    "        '2': [],\n",
    "        '3': [],\n",
    "        '4': [],\n",
    "        '5': [],\n",
    "        '6': []\n",
    "    }\n",
    "\n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Section_Content'] is not None:\n",
    "            section_content_len['1'].append(len(leaflet['Section_1']['Section_Content']))\n",
    "        \n",
    "        if leaflet['Section_2']['Section_Content'] is not None:\n",
    "            section_content_len['2'].append(len(leaflet['Section_2']['Section_Content']))\n",
    "        \n",
    "        if leaflet['Section_3']['Section_Content'] is not None:\n",
    "            section_content_len['3'].append(len(leaflet['Section_3']['Section_Content']))\n",
    "        \n",
    "        if leaflet['Section_4']['Section_Content'] is not None:\n",
    "            section_content_len['4'].append(len(leaflet['Section_4']['Section_Content']))\n",
    "        \n",
    "        if leaflet['Section_5']['Section_Content'] is not None:\n",
    "            section_content_len['5'].append(len(leaflet['Section_5']['Section_Content']))\n",
    "        \n",
    "        if leaflet['Section_6']['Section_Content'] is not None:\n",
    "            section_content_len['6'].append(len(leaflet['Section_6']['Section_Content']))\n",
    "            \n",
    "    print('Section 1: ', np.mean(section_content_len['1']))\n",
    "    print('Section 2: ', np.mean(section_content_len['2']))\n",
    "    print('Section 3: ', np.mean(section_content_len['3']))\n",
    "    print('Section 4: ', np.mean(section_content_len['4']))\n",
    "    print('Section 5: ', np.mean(section_content_len['5']))\n",
    "    print('Section 6: ', np.mean(section_content_len['6']))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print('Section 1 Num. Samples: ', len(section_content_len['1']))\n",
    "    print('Section 2 Num. Samples: ', len(section_content_len['2']))\n",
    "    print('Section 3 Num. Samples: ', len(section_content_len['3']))\n",
    "    print('Section 4 Num. Samples: ', len(section_content_len['4']))\n",
    "    print('Section 5 Num. Samples: ', len(section_content_len['5']))\n",
    "    print('Section 6 Num. Samples: ', len(section_content_len['6']))\n",
    "\n",
    "    # return section_content_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f78c6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1:  962.7945205479452\n",
      "Section 2:  4559.597402597403\n",
      "Section 3:  2300.4912414318355\n",
      "Section 4:  3452.67722007722\n",
      "Section 5:  630.4846416382253\n",
      "Section 6:  981.7040427154843\n",
      "\n",
      "Section 1 Num. Samples:  1314\n",
      "Section 2 Num. Samples:  1309\n",
      "Section 3 Num. Samples:  1313\n",
      "Section 4 Num. Samples:  1295\n",
      "Section 5 Num. Samples:  1172\n",
      "Section 6 Num. Samples:  1311\n"
     ]
    }
   ],
   "source": [
    "calc_section_len(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52d60d",
   "metadata": {},
   "source": [
    "*Numbers calculated before:    *\n",
    "\n",
    "Section 1:  962.7945205479452    \n",
    "Section 2:  4559.597402597403    \n",
    "Section 3:  2300.4912414318355   \n",
    "Section 4:  3452.67722007722   \n",
    "Section 5:  630.4846416382253   \n",
    "Section 6:  981.7040427154843   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f640c5",
   "metadata": {},
   "source": [
    "section 1 ”Therapeu-tic Indication”, which is 1 019 characters lon gon average.    \n",
    "section 4 ”Possible side effects” (3 488 characters longon average) generation is poor    \n",
    "\n",
    "**Explanation** - before I was calculating length on tokenized text just for test_dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3ef2a",
   "metadata": {},
   "source": [
    "### Calc Average Len in tokens - split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e44802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tokens_split(dataset):\n",
    "\n",
    "    # calculate length of each section in tokens\n",
    "\n",
    "    section_content_split = {\n",
    "        '1': [],\n",
    "        '2': [],\n",
    "        '3': [],\n",
    "        '4': [],\n",
    "        '5': [],\n",
    "        '6': []\n",
    "    }\n",
    "\n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Section_Content'] is not None:\n",
    "            # array of tokens\n",
    "            section_content_array = leaflet['Section_1']['Section_Content'].split()\n",
    "            section_content_split['1'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_2']['Section_Content'] is not None:\n",
    "            section_content_array = leaflet['Section_2']['Section_Content'].split()\n",
    "            section_content_split['2'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_3']['Section_Content'] is not None:\n",
    "            section_content_array = leaflet['Section_3']['Section_Content'].split()\n",
    "            section_content_split['3'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_4']['Section_Content'] is not None:\n",
    "            section_content_array = leaflet['Section_4']['Section_Content'].split()\n",
    "            section_content_split['4'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_5']['Section_Content'] is not None:\n",
    "            section_content_array = leaflet['Section_5']['Section_Content'].split()\n",
    "            section_content_split['5'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_6']['Section_Content'] is not None:\n",
    "            section_content_array = leaflet['Section_6']['Section_Content'].split()\n",
    "            section_content_split['6'].append(len(section_content_array))\n",
    "            \n",
    "    print('Section 1: ', np.mean(section_content_split['1']))\n",
    "    print('Section 2: ', np.mean(section_content_split['2']))\n",
    "    print('Section 3: ', np.mean(section_content_split['3']))\n",
    "    print('Section 4: ', np.mean(section_content_split['4']))\n",
    "    print('Section 5: ', np.mean(section_content_split['5']))\n",
    "    print('Section 6: ', np.mean(section_content_split['6']))\n",
    "    \n",
    "    # assert lengh of arrays == already calculated values\n",
    "    assert len(section_content_split['1']) == 1314\n",
    "    assert len(section_content_split['2']) == 1309\n",
    "    assert len(section_content_split['3']) == 1313\n",
    "    assert len(section_content_split['4']) == 1295\n",
    "    assert len(section_content_split['5']) == 1172\n",
    "    assert len(section_content_split['6']) == 1311\n",
    "    \n",
    "    # return section_content_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a22a1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1:  152.68645357686452\n",
      "Section 2:  736.1229946524064\n",
      "Section 3:  407.8926123381569\n",
      "Section 4:  546.4277992277993\n",
      "Section 5:  111.63054607508532\n",
      "Section 6:  152.7398932112891\n"
     ]
    }
   ],
   "source": [
    "calc_tokens_split(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a505c2",
   "metadata": {},
   "source": [
    "### Calc Average Len in tokens - wordpunct_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45d7fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def calc_tokens_wordpunct_tokenize(dataset):\n",
    "\n",
    "    # calculate length of each section in tokens\n",
    "\n",
    "    section_content_wordpunct_token = {\n",
    "        '1': [],\n",
    "        '2': [],\n",
    "        '3': [],\n",
    "        '4': [],\n",
    "        '5': [],\n",
    "        '6': []\n",
    "    }\n",
    "\n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Section_Content'] is not None:\n",
    "            # array of tokens\n",
    "            section_content_array = wordpunct_tokenize(leaflet['Section_1']['Section_Content'])\n",
    "            section_content_wordpunct_token['1'].append(len(section_content_array))\n",
    "            \n",
    "        if leaflet['Section_2']['Section_Content'] is not None:\n",
    "            section_content_array = wordpunct_tokenize(leaflet['Section_2']['Section_Content'])\n",
    "            section_content_wordpunct_token['2'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_3']['Section_Content'] is not None:\n",
    "            section_content_array = wordpunct_tokenize(leaflet['Section_3']['Section_Content'])\n",
    "            section_content_wordpunct_token['3'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_4']['Section_Content'] is not None:\n",
    "            section_content_array = wordpunct_tokenize(leaflet['Section_4']['Section_Content'])\n",
    "            section_content_wordpunct_token['4'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_5']['Section_Content'] is not None:\n",
    "            section_content_array = wordpunct_tokenize(leaflet['Section_5']['Section_Content'])\n",
    "            section_content_wordpunct_token['5'].append(len(section_content_array))\n",
    "        \n",
    "        if leaflet['Section_6']['Section_Content'] is not None:\n",
    "            section_content_array = wordpunct_tokenize(leaflet['Section_6']['Section_Content'])\n",
    "            section_content_wordpunct_token['6'].append(len(section_content_array))\n",
    "            \n",
    "    print('Section 1: ', np.mean(section_content_wordpunct_token['1']))\n",
    "    print('Section 2: ', np.mean(section_content_wordpunct_token['2']))\n",
    "    print('Section 3: ', np.mean(section_content_wordpunct_token['3']))\n",
    "    print('Section 4: ', np.mean(section_content_wordpunct_token['4']))\n",
    "    print('Section 5: ', np.mean(section_content_wordpunct_token['5']))\n",
    "    print('Section 6: ', np.mean(section_content_wordpunct_token['6']))\n",
    "    \n",
    "    # assert lengh of arrays == already calculated values\n",
    "    assert len(section_content_wordpunct_token['1']) == 1314\n",
    "    assert len(section_content_wordpunct_token['2']) == 1309\n",
    "    assert len(section_content_wordpunct_token['3']) == 1313\n",
    "    assert len(section_content_wordpunct_token['4']) == 1295\n",
    "    assert len(section_content_wordpunct_token['5']) == 1172\n",
    "    assert len(section_content_wordpunct_token['6']) == 1311\n",
    "        \n",
    "    # return section_content_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "665920da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1:  174.6986301369863\n",
      "Section 2:  849.7158135981665\n",
      "Section 3:  458.44325971058646\n",
      "Section 4:  651.03861003861\n",
      "Section 5:  123.88737201365188\n",
      "Section 6:  196.86803966437833\n"
     ]
    }
   ],
   "source": [
    "calc_tokens_wordpunct_tokenize(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4b76e",
   "metadata": {},
   "source": [
    "### Calc Average number of entities per section  \n",
    "\n",
    "\n",
    "For the dataset table, I think it would be very useful to include the entity types (e.g. diseases etc) and a break down on how often they appear. (This would help the readers to understand which types of entities are included, and how many.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "605c8cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Text': 'incivo', 'Type': 'PRODUCT_NAME', 'BeginOffset': 0, 'EndOffset': 0},\n",
       " {'Text': 'the virus', 'Type': 'PROBLEM', 'BeginOffset': 20, 'EndOffset': 29},\n",
       " {'Id': 5,\n",
       "  'BeginOffset': 42,\n",
       "  'EndOffset': 63,\n",
       "  'Score': 0.9415066838264465,\n",
       "  'Text': 'hepatitis c infection',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'DIAGNOSIS', 'Score': 0.9677639603614807}]},\n",
       " {'Text': 'chronic hepatitis c infection',\n",
       "  'Type': 'PROBLEM',\n",
       "  'BeginOffset': 85,\n",
       "  'EndOffset': 114},\n",
       " {'Id': 13,\n",
       "  'BeginOffset': 139,\n",
       "  'EndOffset': 143,\n",
       "  'Score': 0.21955788135528564,\n",
       "  'Text': '1865',\n",
       "  'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
       "  'Type': 'AGE',\n",
       "  'Traits': []},\n",
       " {'Id': 0,\n",
       "  'BeginOffset': 171,\n",
       "  'EndOffset': 189,\n",
       "  'Score': 0.92701256275177,\n",
       "  'Text': 'peginterferon alfa',\n",
       "  'Category': 'MEDICATION',\n",
       "  'Type': 'GENERIC_NAME',\n",
       "  'Traits': []},\n",
       " {'Id': 1,\n",
       "  'BeginOffset': 194,\n",
       "  'EndOffset': 203,\n",
       "  'Score': 0.9990363121032715,\n",
       "  'Text': 'ribavirin',\n",
       "  'Category': 'MEDICATION',\n",
       "  'Type': 'GENERIC_NAME',\n",
       "  'Traits': []},\n",
       " {'Id': 2,\n",
       "  'BeginOffset': 240,\n",
       "  'EndOffset': 250,\n",
       "  'Score': 0.7024818062782288,\n",
       "  'Text': 'telaprevir',\n",
       "  'Category': 'MEDICATION',\n",
       "  'Type': 'GENERIC_NAME',\n",
       "  'Traits': []},\n",
       " {'Text': 'medicines',\n",
       "  'Type': 'TREATMENT',\n",
       "  'BeginOffset': 277,\n",
       "  'EndOffset': 286},\n",
       " {'Id': 14,\n",
       "  'BeginOffset': 295,\n",
       "  'EndOffset': 321,\n",
       "  'Score': 0.377422571182251,\n",
       "  'Text': 'ns3-4a protease inhibitors',\n",
       "  'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "  'Type': 'TREATMENT_NAME',\n",
       "  'Traits': []},\n",
       " {'Text': 'the ns3-4a protease inhibitor',\n",
       "  'Type': 'TREATMENT',\n",
       "  'BeginOffset': 324,\n",
       "  'EndOffset': 353},\n",
       " {'Id': 8,\n",
       "  'BeginOffset': 376,\n",
       "  'EndOffset': 393,\n",
       "  'Score': 0.9521518349647522,\n",
       "  'Text': 'hepatitis c virus',\n",
       "  'Category': 'MEDICAL_CONDITION',\n",
       "  'Type': 'DX_NAME',\n",
       "  'Traits': [{'Name': 'DIAGNOSIS', 'Score': 0.9362481236457825}]},\n",
       " {'Id': 3,\n",
       "  'BeginOffset': 477,\n",
       "  'EndOffset': 495,\n",
       "  'Score': 0.9503911137580872,\n",
       "  'Text': 'peginterferon alfa',\n",
       "  'Category': 'MEDICATION',\n",
       "  'Type': 'GENERIC_NAME',\n",
       "  'Traits': []},\n",
       " {'Id': 4,\n",
       "  'BeginOffset': 500,\n",
       "  'EndOffset': 509,\n",
       "  'Score': 0.9992302656173706,\n",
       "  'Text': 'ribavirin',\n",
       "  'Category': 'MEDICATION',\n",
       "  'Type': 'GENERIC_NAME',\n",
       "  'Traits': []},\n",
       " {'Text': 'incivo', 'Type': 'TREATMENT', 'BeginOffset': 543, 'EndOffset': 549},\n",
       " {'Text': 'chronic hepatitis c infection',\n",
       "  'Type': 'PROBLEM',\n",
       "  'BeginOffset': 580,\n",
       "  'EndOffset': 609},\n",
       " {'Text': 'chronic hepatitis c infection',\n",
       "  'Type': 'PROBLEM',\n",
       "  'BeginOffset': 677,\n",
       "  'EndOffset': 706},\n",
       " {'Id': 17,\n",
       "  'BeginOffset': 729,\n",
       "  'EndOffset': 739,\n",
       "  'Score': 0.9999984502792358,\n",
       "  'Text': 'previously',\n",
       "  'Category': 'TIME_EXPRESSION',\n",
       "  'Type': 'TIME_TO_TREATMENT_NAME',\n",
       "  'Traits': [],\n",
       "  'Attributes': [{'Type': 'TREATMENT_NAME',\n",
       "    'Score': 0.8339219689369202,\n",
       "    'RelationshipScore': 0.7144049406051636,\n",
       "    'RelationshipType': 'OVERLAP',\n",
       "    'Id': 16,\n",
       "    'BeginOffset': 748,\n",
       "    'EndOffset': 772,\n",
       "    'Text': 'interferon-based regimen',\n",
       "    'Category': 'TEST_TREATMENT_PROCEDURE',\n",
       "    'Traits': []}]},\n",
       " {'Text': 'an interferon-based regimen',\n",
       "  'Type': 'TREATMENT',\n",
       "  'BeginOffset': 745,\n",
       "  'EndOffset': 772}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[33]['Section_1']['Entity_Recognition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e4f7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entities_type_section(dataset):\n",
    "    \n",
    "    # calculate entity_type - num occurences per section\n",
    "    \n",
    "    # num entities per section\n",
    "    section_entities_num = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "        '4': 0,\n",
    "        '5': 0,\n",
    "        '6': 0\n",
    "    }\n",
    "    \n",
    "    # 'entity type': num\n",
    "    section_entities_type = {\n",
    "        '1': {},\n",
    "        '2': {},\n",
    "        '3': {},\n",
    "        '4': {},\n",
    "        '5': {},\n",
    "        '6': {}\n",
    "    }\n",
    "    \n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Entity_Recognition'] is not None:\n",
    "            \n",
    "            for entity in leaflet['Section_1']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                # number of entities per section\n",
    "                section_entities_num['1'] += 1\n",
    "                \n",
    "                # keep track of entity_type and num. of occurences\n",
    "                if entity_type in section_entities_type['1']:\n",
    "                    section_entities_type['1'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['1'][entity_type] = 1\n",
    "        \n",
    "            \n",
    "        if leaflet['Section_2']['Entity_Recognition'] is not None:\n",
    "            \n",
    "            for entity in leaflet['Section_2']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['2'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['2']:\n",
    "                    section_entities_type['2'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['2'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_3']['Entity_Recognition'] is not None:\n",
    "            \n",
    "            for entity in leaflet['Section_3']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['3'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['3']:\n",
    "                    section_entities_type['3'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['3'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_4']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_4']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['4'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['4']:\n",
    "                    section_entities_type['4'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['4'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_5']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_5']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['5'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['5']:\n",
    "                    section_entities_type['5'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['5'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_6']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_6']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['6'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['6']:\n",
    "                    section_entities_type['6'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['6'][entity_type] = 1\n",
    "    \n",
    "    print(\"Entities count per section\")\n",
    "    print(section_entities_num)\n",
    "    \n",
    "    print(\"\\n\\n Entities type per section\")\n",
    "    print(section_entities_type)\n",
    "    \n",
    "    \n",
    "    # check\n",
    "    assert(section_entities_num['1']) == sum(list(section_entities_type['1'].values()))\n",
    "    assert(section_entities_num['2']) == sum(list(section_entities_type['2'].values()))\n",
    "    assert(section_entities_num['3']) == sum(list(section_entities_type['3'].values()))\n",
    "    assert(section_entities_num['4']) == sum(list(section_entities_type['4'].values()))\n",
    "    assert(section_entities_num['5']) == sum(list(section_entities_type['5'].values()))\n",
    "    assert(section_entities_num['6']) == sum(list(section_entities_type['6'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46bf3975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities count per section\n",
      "{'1': 38542, '2': 167271, '3': 66304, '4': 175021, '5': 7341, '6': 50297}\n",
      "\n",
      "\n",
      " Entities type per section\n",
      "{'1': {'PRODUCT_NAME': 1321, 'PROBLEM': 8850, 'SYSTEM_ORGAN_SITE': 2588, 'DX_NAME': 8363, 'TEST': 1215, 'TREATMENT_NAME': 1249, 'GENERIC_NAME': 3895, 'TREATMENT': 7624, 'BRAND_NAME': 1131, 'NAME': 104, 'NUMBER': 1132, 'TEST_NAME': 370, 'PROCEDURE_NAME': 249, 'TIME_TO_DX_NAME': 92, 'DATE': 12, 'AGE': 235, 'TIME_TO_TREATMENT_NAME': 30, 'TIME_TO_MEDICATION_NAME': 31, 'ADDRESS': 20, 'TIME_TO_PROCEDURE_NAME': 14, 'TIME_TO_TEST_NAME': 3, 'ID': 4, 'PHONE_OR_FAX': 8, 'PROFESSION': 1, 'URL': 1}, '2': {'PRODUCT_NAME': 1321, 'TREATMENT': 38334, 'BRAND_NAME': 6240, 'PROBLEM': 25147, 'DX_NAME': 44819, 'GENERIC_NAME': 25509, 'NUMBER': 6073, 'TREATMENT_NAME': 5286, 'SYSTEM_ORGAN_SITE': 5191, 'PROCEDURE_NAME': 722, 'TIME_TO_PROCEDURE_NAME': 97, 'TEST': 3875, 'AGE': 530, 'TIME_TO_TREATMENT_NAME': 395, 'TIME_TO_MEDICATION_NAME': 562, 'TIME_TO_DX_NAME': 1374, 'TEST_NAME': 1585, 'TIME_TO_TEST_NAME': 108, 'ADDRESS': 44, 'NAME': 23, 'DATE': 11, 'PHONE_OR_FAX': 5, 'EMAIL': 17, 'ID': 3}, '3': {'PRODUCT_NAME': 1325, 'TREATMENT': 21173, 'BRAND_NAME': 3698, 'TEST': 1460, 'DX_NAME': 6189, 'NUMBER': 15033, 'TIME_TO_MEDICATION_NAME': 730, 'TREATMENT_NAME': 1219, 'PROBLEM': 4568, 'SYSTEM_ORGAN_SITE': 3738, 'TIME_TO_TREATMENT_NAME': 284, 'PROCEDURE_NAME': 614, 'GENERIC_NAME': 5081, 'ADDRESS': 189, 'TEST_NAME': 459, 'TIME_TO_TEST_NAME': 29, 'TIME_TO_DX_NAME': 148, 'TIME_TO_PROCEDURE_NAME': 163, 'AGE': 149, 'PROFESSION': 15, 'DATE': 12, 'NAME': 15, 'ID': 12, 'PHONE_OR_FAX': 1}, '4': {'PRODUCT_NAME': 1305, 'TREATMENT': 9177, 'DX_NAME': 87625, 'PROBLEM': 44526, 'TREATMENT_NAME': 824, 'BRAND_NAME': 900, 'NUMBER': 13828, 'SYSTEM_ORGAN_SITE': 8063, 'PROCEDURE_NAME': 123, 'TEST': 2427, 'TEST_NAME': 2555, 'GENERIC_NAME': 2250, 'TIME_TO_DX_NAME': 913, 'TIME_TO_TREATMENT_NAME': 38, 'TIME_TO_MEDICATION_NAME': 122, 'ADDRESS': 110, 'AGE': 53, 'DATE': 10, 'TIME_TO_TEST_NAME': 20, 'TIME_TO_PROCEDURE_NAME': 8, 'ID': 24, 'NAME': 71, 'PHONE_OR_FAX': 41, 'PROFESSION': 1, 'EMAIL': 6, 'URL': 1}, '5': {'PRODUCT_NAME': 864, 'TREATMENT': 3443, 'NUMBER': 1821, 'PROBLEM': 607, 'TREATMENT_NAME': 45, 'GENERIC_NAME': 82, 'TIME_TO_MEDICATION_NAME': 31, 'TIME_TO_TREATMENT_NAME': 15, 'DX_NAME': 167, 'BRAND_NAME': 112, 'TIME_TO_PROCEDURE_NAME': 7, 'PROCEDURE_NAME': 17, 'TEST': 36, 'TEST_NAME': 16, 'TIME_TO_TEST_NAME': 3, 'ADDRESS': 20, 'NAME': 6, 'AGE': 2, 'ID': 4, 'PHONE_OR_FAX': 8, 'DATE': 1, 'SYSTEM_ORGAN_SITE': 32, 'TIME_TO_DX_NAME': 2}, '6': {'PRODUCT_NAME': 1314, 'GENERIC_NAME': 13970, 'NUMBER': 17842, 'TREATMENT': 12418, 'PROBLEM': 1478, 'BRAND_NAME': 1331, 'TEST': 919, 'TEST_NAME': 24, 'DX_NAME': 573, 'SYSTEM_ORGAN_SITE': 206, 'ID': 11, 'TIME_TO_MEDICATION_NAME': 22, 'TREATMENT_NAME': 55, 'NAME': 3, 'DATE': 68, 'PROCEDURE_NAME': 14, 'TIME_TO_TREATMENT_NAME': 3, 'AGE': 10, 'TIME_TO_DX_NAME': 10, 'ADDRESS': 24, 'TIME_TO_PROCEDURE_NAME': 2}}\n"
     ]
    }
   ],
   "source": [
    "calc_entities_type_section(whole_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68dcef",
   "metadata": {},
   "source": [
    "### Number unique entities per section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "274a961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unique_entities_section(dataset):\n",
    "    \n",
    "    # calculate unique entities per section\n",
    "    \n",
    "    # unique entities\n",
    "    section_unique_entities = {\n",
    "        '1': set(),\n",
    "        '2': set(),\n",
    "        '3': set(),\n",
    "        '4': set(),\n",
    "        '5': set(),\n",
    "        '6': set()\n",
    "    }\n",
    "    \n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_1']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                section_unique_entities['1'].add(entity_text)\n",
    "            \n",
    "        if leaflet['Section_2']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_2']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                section_unique_entities['2'].add(entity_text)\n",
    "        \n",
    "        if leaflet['Section_3']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_3']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                section_unique_entities['3'].add(entity_text)\n",
    "        \n",
    "        if leaflet['Section_4']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_4']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                section_unique_entities['4'].add(entity_text)\n",
    "        \n",
    "        if leaflet['Section_5']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_5']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                section_unique_entities['5'].add(entity_text)\n",
    "        \n",
    "        if leaflet['Section_6']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_6']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                section_unique_entities['6'].add(entity_text)\n",
    "    \n",
    "    for section_type in section_unique_entities:\n",
    "        print(section_type, \" = \", len(section_unique_entities[section_type]))\n",
    "    \n",
    "    \n",
    "    # check ? \n",
    "    return section_unique_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32485854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  =  9641\n",
      "2  =  23278\n",
      "3  =  11640\n",
      "4  =  27945\n",
      "5  =  2041\n",
      "6  =  9932\n"
     ]
    }
   ],
   "source": [
    "bla = calc_unique_entities_section(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bffa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0185965",
   "metadata": {},
   "source": [
    "1  =  9641\n",
    "2  =  23278\n",
    "3  =  11640\n",
    "4  =  27945\n",
    "5  =  2041\n",
    "6  =  9932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09e7af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unique_entities_section(dataset):\n",
    "    \n",
    "    ### main difference - now I can actualy see the most popular entities per each section\n",
    "    \n",
    "    # calculate unique entities per section\n",
    "    \n",
    "    # entity - num. it appears\n",
    "    section_unique_entities = {\n",
    "        '1': dict(),\n",
    "        '2': dict(),\n",
    "        '3': dict(),\n",
    "        '4': dict(),\n",
    "        '5': dict(),\n",
    "        '6': dict()\n",
    "    }\n",
    "    \n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_1']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                \n",
    "                if entity_text in section_unique_entities['1']:\n",
    "                    section_unique_entities['1'][entity_text] += 1\n",
    "                else:\n",
    "                    section_unique_entities['1'][entity_text] = 1\n",
    "            \n",
    "        if leaflet['Section_2']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_2']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                if entity_text in section_unique_entities['2']:\n",
    "                    section_unique_entities['2'][entity_text] += 1\n",
    "                else:\n",
    "                    section_unique_entities['2'][entity_text] = 1\n",
    "        \n",
    "        if leaflet['Section_3']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_3']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                if entity_text in section_unique_entities['3']:\n",
    "                    section_unique_entities['3'][entity_text] += 1\n",
    "                else:\n",
    "                    section_unique_entities['3'][entity_text] = 1\n",
    "        \n",
    "        if leaflet['Section_4']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_4']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                if entity_text in section_unique_entities['4']:\n",
    "                    section_unique_entities['4'][entity_text] += 1\n",
    "                else:\n",
    "                    section_unique_entities['4'][entity_text] = 1\n",
    "        \n",
    "        if leaflet['Section_5']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_5']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                if entity_text in section_unique_entities['5']:\n",
    "                    section_unique_entities['5'][entity_text] += 1\n",
    "                else:\n",
    "                    section_unique_entities['5'][entity_text] = 1\n",
    "        \n",
    "        if leaflet['Section_6']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_6']['Entity_Recognition']:\n",
    "                entity_text = entity['Text']\n",
    "                if entity_text in section_unique_entities['6']:\n",
    "                    section_unique_entities['6'][entity_text] += 1\n",
    "                else:\n",
    "                    section_unique_entities['6'][entity_text] = 1\n",
    "    \n",
    "    for section_type in section_unique_entities:\n",
    "        \n",
    "        total_num_entities = 0\n",
    "        for entity_text in section_unique_entities[section_type]:\n",
    "            total_num_entities += section_unique_entities[section_type][entity_text]\n",
    "        \n",
    "        print(section_type, \" ~ Num. unique entities: \", len(section_unique_entities[section_type]))\n",
    "        print(section_type, \" ~ Total entities: \", total_num_entities)\n",
    "        print()\n",
    "        \n",
    "    # but now I can see the most popular entities per each section\n",
    "    # return section_unique_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9bde3daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  ~ Num. unique entities:  9641\n",
      "1  ~ Total entities:  38542\n",
      "\n",
      "2  ~ Num. unique entities:  23278\n",
      "2  ~ Total entities:  167271\n",
      "\n",
      "3  ~ Num. unique entities:  11640\n",
      "3  ~ Total entities:  66304\n",
      "\n",
      "4  ~ Num. unique entities:  27945\n",
      "4  ~ Total entities:  175021\n",
      "\n",
      "5  ~ Num. unique entities:  2041\n",
      "5  ~ Total entities:  7341\n",
      "\n",
      "6  ~ Num. unique entities:  9932\n",
      "6  ~ Total entities:  50297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calc_unique_entities_section(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f11b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b2c6b68",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "### TYPO in section 6 - What I was calculating before:\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91834269",
   "metadata": {},
   "source": [
    "1019.984962406015   \n",
    "3488.5193798449613    \n",
    "\n",
    "section 1: <PRODUCT_NAME> incivo </PRODUCT_NAME> <PROBLEM> the_virus </PROBLEM> <DX_NAME> hepatitis_c_infection </DX_NAME> <PROBLEM> chronic_hepatitis_c_infection </PROBLEM> <AGE> 1865 </AGE> <GENERIC_NAME> peginterferon_alfa </GENERIC_NAME> <GENERIC_NAME> ribavirin </GENERIC_NAME> <GENERIC_NAME> telaprevir </GENERIC_NAME> <TREATMENT> medicines </TREATMENT> <TREATMENT_NAME> ns3-4a_protease_inhibitors </TREATMENT_NAME> <TREATMENT> the_ns3-4a_protease_inhibitor </TREATMENT> <DX_NAME> hepatitis_c_virus </DX_NAME> <GENERIC_NAME> peginterferon_alfa </GENERIC_NAME> <GENERIC_NAME> ribavirin </GENERIC_NAME> <TREATMENT> incivo </TREATMENT> <PROBLEM> chronic_hepatitis_c_infection </PROBLEM> <PROBLEM> chronic_hepatitis_c_infection </PROBLEM>     <TIME_TO_TREATMENT_NAME> previously </TIME_TO_TREATMENT_NAME> <TREATMENT> an_interferon-based_regimen </TREATMENT>\n",
    "incivo acts against the virus that causes hepatitis c infection and is used to treat chronic hepatitis c infection in adult patients ( aged 1865 years ) in combination with peginterferon alfa and ribavirin . incivo contains a substance called telaprevir and belongs to a group of medicines called ' ns3 - 4a protease inhibitors '. the ns3 - 4a protease inhibitor reduces the amount of hepatitis c virus in your body . incivo must not be taken alone and must be taken in combination with peginterferon alfa and ribavirin to be sure your treatment works . incivo can be used for patients with chronic hepatitis c infection who have never been treated before or can be used in patients with chronic hepatitis c infection who have been treated previously with an interferon - based regimen .   \n",
    "\n",
    "1019.984962406015    \n",
    "3488.5193798449613   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b686134",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "\n",
    "PATH='/content/drive/MyDrive/bayer-intern/content_planner_generations/section1_tgt_test.txt'\n",
    "\n",
    "# read T5 reference sections\n",
    "with open(PATH) as f:\n",
    "    section1_ref = [line.strip() for line in f]\n",
    "\n",
    "length_1 = []\n",
    "for i in section1_ref:\n",
    "  length_1.append(len(i))\n",
    "\n",
    "print(np.mean(length_1))\n",
    "\n",
    "PATH='/content/drive/MyDrive/bayer-intern/content_planner_generations/section4_tgt_test.txt'\n",
    "\n",
    "# read T5 reference sections\n",
    "with open(PATH) as f:\n",
    "    section4_ref = [line.strip() for line in f]\n",
    "\n",
    "length_4 = []\n",
    "for i in section4_ref:\n",
    "  length_4.append(len(i))\n",
    "\n",
    "print(np.mean(length_4))\n",
    "\n",
    "## ----------------------------\n",
    "PATH = '/content/drive/MyDrive/bayer-intern/T5_condition_input/input_data/test.source'\n",
    "# read T5 reference sections\n",
    "with open(PATH) as f:\n",
    "    input = [line.strip() for line in f]\n",
    "\n",
    "PATH = '/content/drive/MyDrive/bayer-intern/T5_condition_input/input_data/test.target'\n",
    "# read T5 reference sections\n",
    "with open(PATH) as f:\n",
    "    output = [line.strip() for line in f]\n",
    "\n",
    "print(input[0])\n",
    "print(output[0])\n",
    "\n",
    "bla_1 = []\n",
    "bla_4 = []\n",
    "for i in range(len(input)):\n",
    "  if input[i][:9] == 'section 1':\n",
    "    bla_1.append(len(output[i]))\n",
    "  elif input[i][:9] == 'section 4':\n",
    "    bla_4.append(len(output[i]))\n",
    "\n",
    "print(np.mean(bla_1))\n",
    "print(np.mean(bla_4))\n",
    "###########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328a964",
   "metadata": {},
   "source": [
    "Reason: -- because here it is tokenized text for test dataset ??     \n",
    "\n",
    "\n",
    "incivo acts against the virus that causes hepatitis c infection and is used to treat chronic hepatitis c infection in adult patients ( aged 1865 years ) in combination with peginterferon alfa and ribavirin . incivo contains a substance called telaprevir and belongs to a group of medicines called ' ns3 - 4a protease inhibitors '. the ns3 - 4a protease inhibitor reduces the amount of hepatitis c virus in your body . incivo must not be taken alone and must be taken in combination with peginterferon alfa and ribavirin to be sure your treatment works . incivo can be used for patients with chronic hepatitis c infection who have never been treated before or can be used in patients with chronic hepatitis c infection who have been treated previously with an interferon - based regimen ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94957e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def calc_section_len_reproduce_error(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the average length of each section(1-6)\n",
    "    :param package_leaflets: list, collection of package leaflets\n",
    "    :return: dict, (key: str, section_num; value: list of lengths of corresponding sections)\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate length of each section\n",
    "\n",
    "    section_content_len = {\n",
    "        '1': [],\n",
    "        '2': [],\n",
    "        '3': [],\n",
    "        '4': [],\n",
    "        '5': [],\n",
    "        '6': []\n",
    "    }\n",
    "\n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Section_Content'] is not None:\n",
    "            section_content = wordpunct_tokenize(leaflet['Section_1']['Section_Content'])\n",
    "            section_content = \" \".join(section_content)\n",
    "            # section_content = section_content + \"\\n\"\n",
    "            section_content_len['1'].append(len(section_content))\n",
    "        \n",
    "        if leaflet['Section_2']['Section_Content'] is not None:\n",
    "            section_content = wordpunct_tokenize(leaflet['Section_2']['Section_Content'])\n",
    "            section_content = \" \".join(section_content)\n",
    "            # section_content = section_content + \"\\n\"\n",
    "            section_content_len['2'].append(len(section_content))\n",
    "        \n",
    "        if leaflet['Section_3']['Section_Content'] is not None:\n",
    "            section_content = wordpunct_tokenize(leaflet['Section_3']['Section_Content'])\n",
    "            section_content = \" \".join(section_content)\n",
    "            # section_content = section_content + \"\\n\"\n",
    "            section_content_len['3'].append(len(section_content))\n",
    "        \n",
    "        if leaflet['Section_4']['Section_Content'] is not None:\n",
    "            section_content = wordpunct_tokenize(leaflet['Section_4']['Section_Content'])\n",
    "            section_content = \" \".join(section_content)\n",
    "            # section_content = section_content + \"\\n\"\n",
    "            section_content_len['4'].append(len(section_content))\n",
    "        \n",
    "        if leaflet['Section_5']['Section_Content'] is not None:\n",
    "            section_content = wordpunct_tokenize(leaflet['Section_5']['Section_Content'])\n",
    "            section_content = \" \".join(section_content)\n",
    "            # section_content = section_content + \"\\n\"\n",
    "            section_content_len['5'].append(len(section_content))\n",
    "        \n",
    "        if leaflet['Section_6']['Section_Content'] is not None:\n",
    "            section_content = wordpunct_tokenize(leaflet['Section_6']['Section_Content'])\n",
    "            section_content = \" \".join(section_content)\n",
    "            # section_content = section_content + \"\\n\"\n",
    "            section_content_len['6'].append(len(section_content))\n",
    "            \n",
    "    print('Section 1: ', np.mean(section_content_len['1']))\n",
    "    print('Section 2: ', np.mean(section_content_len['2']))\n",
    "    print('Section 3: ', np.mean(section_content_len['3']))\n",
    "    print('Section 4: ', np.mean(section_content_len['4']))\n",
    "    print('Section 5: ', np.mean(section_content_len['5']))\n",
    "    print('Section 6: ', np.mean(section_content_len['6']))\n",
    "\n",
    "    # return section_content_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f34cd1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1:  1019.984962406015\n",
      "Section 2:  4465.187969924812\n",
      "Section 3:  2265.2631578947367\n",
      "Section 4:  3488.5193798449613\n",
      "Section 5:  642.7217391304348\n",
      "Section 6:  1022.6315789473684\n"
     ]
    }
   ],
   "source": [
    "calc_section_len_reproduce_error(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09f175",
   "metadata": {},
   "source": [
    "1019.984962406015    \n",
    "3488.5193798449613    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e73b98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incivo acts against the virus that causes hepatitis c infection and is used to treat chronic hepatitis c infection in adult patients (aged 1865 years) in combination with peginterferon alfa and ribavirin. incivo contains a substance called telaprevir and belongs to a group of medicines called 'ns3-4a protease inhibitors'. the ns3-4a protease inhibitor reduces the amount of hepatitis c virus in your body. incivo must not be taken alone and must be taken in combination with peginterferon alfa and ribavirin to be sure your treatment works. incivo can be used for patients with chronic hepatitis c infection who have never been treated before or can be used in patients with chronic hepatitis c infection who have been treated previously with an interferon-based regimen.\n",
      "\n",
      "33\n",
      "\n",
      "['incivo', 'acts', 'against', 'the', 'virus', 'that', 'causes', 'hepatitis', 'c', 'infection', 'and', 'is', 'used', 'to', 'treat', 'chronic', 'hepatitis', 'c', 'infection', 'in', 'adult', 'patients', '(', 'aged', '1865', 'years', ')', 'in', 'combination', 'with', 'peginterferon', 'alfa', 'and', 'ribavirin', '.', 'incivo', 'contains', 'a', 'substance', 'called', 'telaprevir', 'and', 'belongs', 'to', 'a', 'group', 'of', 'medicines', 'called', \"'\", 'ns3', '-', '4a', 'protease', 'inhibitors', \"'.\", 'the', 'ns3', '-', '4a', 'protease', 'inhibitor', 'reduces', 'the', 'amount', 'of', 'hepatitis', 'c', 'virus', 'in', 'your', 'body', '.', 'incivo', 'must', 'not', 'be', 'taken', 'alone', 'and', 'must', 'be', 'taken', 'in', 'combination', 'with', 'peginterferon', 'alfa', 'and', 'ribavirin', 'to', 'be', 'sure', 'your', 'treatment', 'works', '.', 'incivo', 'can', 'be', 'used', 'for', 'patients', 'with', 'chronic', 'hepatitis', 'c', 'infection', 'who', 'have', 'never', 'been', 'treated', 'before', 'or', 'can', 'be', 'used', 'in', 'patients', 'with', 'chronic', 'hepatitis', 'c', 'infection', 'who', 'have', 'been', 'treated', 'previously', 'with', 'an', 'interferon', '-', 'based', 'regimen', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in test_dataset:\n",
    "    if i['Product_Name'] == 'Incivo':\n",
    "        print(i['Section_1']['Section_Content'])\n",
    "        print()\n",
    "        print(test_dataset.index(i))\n",
    "        print()\n",
    "        print(wordpunct_tokenize(i['Section_1']['Section_Content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d1294",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0430cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entities_type_section(dataset):\n",
    "    \n",
    "    # calculate entity_type - num occurences per section\n",
    "    \n",
    "    # num entities per section\n",
    "    section_entities_num = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "        '4': 0,\n",
    "        '5': 0,\n",
    "        '6': 0\n",
    "    }\n",
    "    \n",
    "    # 'entity type': num\n",
    "    section_entities_type = {\n",
    "        '1': {},\n",
    "        '2': {},\n",
    "        '3': {},\n",
    "        '4': {},\n",
    "        '5': {},\n",
    "        '6': {}\n",
    "    }\n",
    "    \n",
    "    # calc the length of section content and add to list\n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        if leaflet['Section_1']['Entity_Recognition'] is not None:\n",
    "            \n",
    "            for entity in leaflet['Section_1']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                # number of entities per section\n",
    "                section_entities_num['1'] += 1\n",
    "                \n",
    "                # keep track of entity_type and num. of occurences\n",
    "                if entity_type in section_entities_type['1']:\n",
    "                    section_entities_type['1'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['1'][entity_type] = 1\n",
    "        \n",
    "            \n",
    "        if leaflet['Section_2']['Entity_Recognition'] is not None:\n",
    "            \n",
    "            for entity in leaflet['Section_2']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['2'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['2']:\n",
    "                    section_entities_type['2'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['2'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_3']['Entity_Recognition'] is not None:\n",
    "            \n",
    "            for entity in leaflet['Section_3']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['3'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['3']:\n",
    "                    section_entities_type['3'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['3'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_4']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_4']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['4'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['4']:\n",
    "                    section_entities_type['4'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['4'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_5']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_5']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['5'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['5']:\n",
    "                    section_entities_type['5'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['5'][entity_type] = 1\n",
    "        \n",
    "        if leaflet['Section_6']['Entity_Recognition'] is not None:\n",
    "            for entity in leaflet['Section_6']['Entity_Recognition']:\n",
    "                entity_type = entity['Type']\n",
    "                \n",
    "                section_entities_num['6'] += 1\n",
    "                \n",
    "                if entity_type in section_entities_type['6']:\n",
    "                    section_entities_type['6'][entity_type] += 1\n",
    "                else:\n",
    "                    section_entities_type['6'][entity_type] = 1\n",
    "    \n",
    "    print(\"Entities count per section\")\n",
    "    print(section_entities_num)\n",
    "    \n",
    "    print(\"\\n\\n Entities type per section\")\n",
    "    print(section_entities_type)\n",
    "    \n",
    "    \n",
    "    # check\n",
    "    assert(section_entities_num['1']) == sum(list(section_entities_type['1'].values()))\n",
    "    assert(section_entities_num['2']) == sum(list(section_entities_type['2'].values()))\n",
    "    assert(section_entities_num['3']) == sum(list(section_entities_type['3'].values()))\n",
    "    assert(section_entities_num['4']) == sum(list(section_entities_type['4'].values()))\n",
    "    assert(section_entities_num['5']) == sum(list(section_entities_type['5'].values()))\n",
    "    assert(section_entities_num['6']) == sum(list(section_entities_type['6'].values()))\n",
    "    \n",
    "    \n",
    "    return section_entities_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc731b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities count per section\n",
      "{'1': 38542, '2': 167271, '3': 66304, '4': 175021, '5': 7341, '6': 50297}\n",
      "\n",
      "\n",
      " Entities type per section\n",
      "{'1': {'PRODUCT_NAME': 1321, 'PROBLEM': 8850, 'SYSTEM_ORGAN_SITE': 2588, 'DX_NAME': 8363, 'TEST': 1215, 'TREATMENT_NAME': 1249, 'GENERIC_NAME': 3895, 'TREATMENT': 7624, 'BRAND_NAME': 1131, 'NAME': 104, 'NUMBER': 1132, 'TEST_NAME': 370, 'PROCEDURE_NAME': 249, 'TIME_TO_DX_NAME': 92, 'DATE': 12, 'AGE': 235, 'TIME_TO_TREATMENT_NAME': 30, 'TIME_TO_MEDICATION_NAME': 31, 'ADDRESS': 20, 'TIME_TO_PROCEDURE_NAME': 14, 'TIME_TO_TEST_NAME': 3, 'ID': 4, 'PHONE_OR_FAX': 8, 'PROFESSION': 1, 'URL': 1}, '2': {'PRODUCT_NAME': 1321, 'TREATMENT': 38334, 'BRAND_NAME': 6240, 'PROBLEM': 25147, 'DX_NAME': 44819, 'GENERIC_NAME': 25509, 'NUMBER': 6073, 'TREATMENT_NAME': 5286, 'SYSTEM_ORGAN_SITE': 5191, 'PROCEDURE_NAME': 722, 'TIME_TO_PROCEDURE_NAME': 97, 'TEST': 3875, 'AGE': 530, 'TIME_TO_TREATMENT_NAME': 395, 'TIME_TO_MEDICATION_NAME': 562, 'TIME_TO_DX_NAME': 1374, 'TEST_NAME': 1585, 'TIME_TO_TEST_NAME': 108, 'ADDRESS': 44, 'NAME': 23, 'DATE': 11, 'PHONE_OR_FAX': 5, 'EMAIL': 17, 'ID': 3}, '3': {'PRODUCT_NAME': 1325, 'TREATMENT': 21173, 'BRAND_NAME': 3698, 'TEST': 1460, 'DX_NAME': 6189, 'NUMBER': 15033, 'TIME_TO_MEDICATION_NAME': 730, 'TREATMENT_NAME': 1219, 'PROBLEM': 4568, 'SYSTEM_ORGAN_SITE': 3738, 'TIME_TO_TREATMENT_NAME': 284, 'PROCEDURE_NAME': 614, 'GENERIC_NAME': 5081, 'ADDRESS': 189, 'TEST_NAME': 459, 'TIME_TO_TEST_NAME': 29, 'TIME_TO_DX_NAME': 148, 'TIME_TO_PROCEDURE_NAME': 163, 'AGE': 149, 'PROFESSION': 15, 'DATE': 12, 'NAME': 15, 'ID': 12, 'PHONE_OR_FAX': 1}, '4': {'PRODUCT_NAME': 1305, 'TREATMENT': 9177, 'DX_NAME': 87625, 'PROBLEM': 44526, 'TREATMENT_NAME': 824, 'BRAND_NAME': 900, 'NUMBER': 13828, 'SYSTEM_ORGAN_SITE': 8063, 'PROCEDURE_NAME': 123, 'TEST': 2427, 'TEST_NAME': 2555, 'GENERIC_NAME': 2250, 'TIME_TO_DX_NAME': 913, 'TIME_TO_TREATMENT_NAME': 38, 'TIME_TO_MEDICATION_NAME': 122, 'ADDRESS': 110, 'AGE': 53, 'DATE': 10, 'TIME_TO_TEST_NAME': 20, 'TIME_TO_PROCEDURE_NAME': 8, 'ID': 24, 'NAME': 71, 'PHONE_OR_FAX': 41, 'PROFESSION': 1, 'EMAIL': 6, 'URL': 1}, '5': {'PRODUCT_NAME': 864, 'TREATMENT': 3443, 'NUMBER': 1821, 'PROBLEM': 607, 'TREATMENT_NAME': 45, 'GENERIC_NAME': 82, 'TIME_TO_MEDICATION_NAME': 31, 'TIME_TO_TREATMENT_NAME': 15, 'DX_NAME': 167, 'BRAND_NAME': 112, 'TIME_TO_PROCEDURE_NAME': 7, 'PROCEDURE_NAME': 17, 'TEST': 36, 'TEST_NAME': 16, 'TIME_TO_TEST_NAME': 3, 'ADDRESS': 20, 'NAME': 6, 'AGE': 2, 'ID': 4, 'PHONE_OR_FAX': 8, 'DATE': 1, 'SYSTEM_ORGAN_SITE': 32, 'TIME_TO_DX_NAME': 2}, '6': {'PRODUCT_NAME': 1314, 'GENERIC_NAME': 13970, 'NUMBER': 17842, 'TREATMENT': 12418, 'PROBLEM': 1478, 'BRAND_NAME': 1331, 'TEST': 919, 'TEST_NAME': 24, 'DX_NAME': 573, 'SYSTEM_ORGAN_SITE': 206, 'ID': 11, 'TIME_TO_MEDICATION_NAME': 22, 'TREATMENT_NAME': 55, 'NAME': 3, 'DATE': 68, 'PROCEDURE_NAME': 14, 'TIME_TO_TREATMENT_NAME': 3, 'AGE': 10, 'TIME_TO_DX_NAME': 10, 'ADDRESS': 24, 'TIME_TO_PROCEDURE_NAME': 2}}\n"
     ]
    }
   ],
   "source": [
    "result = calc_entities_type_section(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff48f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types = set()\n",
    "\n",
    "for sec_type in result:\n",
    "    entities_section = result[sec_type]\n",
    "    \n",
    "    for entity_type in entities_section:\n",
    "        num_types.add(entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77f0b739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d28396f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red blood cells', 'the heart', 'gamma gt', 'your blood cells', 'the enzyme', 'your blood or urine', 'a dental examination', 'the vegf protein', 'a monoclonal antibody', 'laboratory tests', 'potassium level', 'your blood levels', 'flushing tests', 'your kidney function', 'your platelet count', 'potassium in your blood', 'regular blood tests', 'the cd4 cell count', 'your blood pressure', 'your blood', 'the blood', 'an enzyme', 'sodium', 'blood tests', 'your urine', 'your last blood test', 'a blood test', 'red blood cell count', 'platelets', 'globotriaosylceramide (gl', 'a pregnancy test', 'your plasma levels', 'a dental check', 'your blood fluid', 'a spinal puncture', 'clinical studies', 'a faulty enzyme', 'fatty acids', 'your cholesterol levels', 'one study'}\n"
     ]
    }
   ],
   "source": [
    "unique_display = set()\n",
    "for i in [0, 100, 500, 300, 333, 777, 1234, 321, 12, -1, 228, 555, 7]:\n",
    "    for entity in whole_dataset[i]['Section_1']['Entity_Recognition']:\n",
    "        if entity['Type'] == 'TEST':\n",
    "            unique_display.add(entity['Text'])\n",
    "            \n",
    "    for entity in whole_dataset[i]['Section_2']['Entity_Recognition']:\n",
    "        if entity['Type'] == 'TEST':\n",
    "            unique_display.add(entity['Text'])\n",
    "            \n",
    "    for entity in whole_dataset[i]['Section_4']['Entity_Recognition']:\n",
    "        if entity['Type'] == 'TEST':\n",
    "            unique_display.add(entity['Text'])\n",
    "\n",
    "print(unique_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa782dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stent in the blocked', 'injections', 'operations', 'spinal or epidural anaesthesia', 'major surgery', 'lumbar puncture', 'anaesthetics', 'injection', 'surgical intervention', 'tooth extractions', 'surgery', 'operations and anesthetics', 'bone marrow or stem cell transplant', 'operation', 'spinal surgery', 'spinal or epidural anesthesia', 'epidural or spinal anaesthetic', 'dental surgery', 'liver transplant'}\n"
     ]
    }
   ],
   "source": [
    "unique_display = set()\n",
    "for i in [0, 100, 500, 300, 333, 777, 1234, 321, 12, -1, 228, 555, 7, 123, 5, 22, 25, 789, 46, 1246]:\n",
    "    for entity in whole_dataset[i]['Section_1']['Entity_Recognition']:\n",
    "        if entity['Type'] == 'PROCEDURE_NAME':\n",
    "            unique_display.add(entity['Text'])\n",
    "            \n",
    "    for entity in whole_dataset[i]['Section_2']['Entity_Recognition']:\n",
    "        if entity['Type'] == 'PROCEDURE_NAME':\n",
    "            unique_display.add(entity['Text'])\n",
    "            \n",
    "    for entity in whole_dataset[i]['Section_4']['Entity_Recognition']:\n",
    "        if entity['Type'] == 'PROCEDURE_NAME':\n",
    "            unique_display.add(entity['Text'])\n",
    "\n",
    "print(unique_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c46cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
